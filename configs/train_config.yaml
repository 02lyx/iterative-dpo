learning_rate: 2.0e-5
num_train_epochs: 5
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 1
gradient_checkpointing: true
bf16: true
max_seq_length: 2048
weight_decay: 0.0
warmup_ratio: 0.1
optim: "adamw_torch"
lr_scheduler_type: "cosine"
push_to_hub: false
attn_implementation: "eager"